{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests, zipfile, io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "## to split dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Citation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dblp.v10.zip'\n",
    "url = 'http://aminer.org/lab-datasets/citation/dblp.v10.zip'\n",
    "\n",
    "path = \"Data\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the Data directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the Data directory %s \" % path)\n",
    "\n",
    "else:\n",
    "    print(\"Data directory already exist.\\n\")\n",
    "\n",
    "print(\"Downloading Citation data into \" + path + \" directory...\")\n",
    "\n",
    "with open('Data/'+filename, 'wb') as f:\n",
    "        response = requests.get(url, stream=True)\n",
    "        total = response.headers.get('content-length')\n",
    "\n",
    "        if total is None:\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            downloaded = 0\n",
    "            total = int(total)\n",
    "            for data in response.iter_content(chunk_size=max(int(total/1000), 1024*1024)):\n",
    "                downloaded += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50*downloaded/total)\n",
    "                sys.stdout.write('\\r[{}{}]'.format('â–ˆ' * done, '.' * (50-done)))\n",
    "                sys.stdout.flush()\n",
    "        sys.stdout.write('\\n Finished Downloading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dblp.v10.zip'\n",
    "path = \"Data\"\n",
    "\n",
    "filepath = path + \"/\" + filename\n",
    "print(\"Extracting Citation data\")\n",
    "with zipfile.ZipFile(filepath) as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting '):\n",
    "            try:\n",
    "                zf.extract(member, path)\n",
    "            except zipfile.error as e:\n",
    "                pass\n",
    "        print(\"Finished Extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting .json file .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/dblp-ref/dblp-ref-3.json', 'r') as data_3:\n",
    "     data3 = data_3.read() \n",
    "del data_3\n",
    "data3 = '[' + data3.replace('}', '},', data3.count('}')-1) + ']'\n",
    "data3_json = json.loads(data3)\n",
    "del data3\n",
    "data3 = pd.DataFrame.from_dict(data3_json, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying filters for case one experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one = data3[data3.year < 2009] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one = data3[data3.year == 2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one.reset_index(drop=True, inplace=True)\n",
    "test_one.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one.to_csv('Data/train_case_one.csv')\n",
    "test_one.to_csv('Data/test_case_one.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average citation count for venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VenueNames = train_one['venue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite = pd.DataFrame(VenueNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.rename(index=str, columns={0: \"venue\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in AvgVenueCite.iterrows():\n",
    "    data = train[train.venue == row.venue]\n",
    "    if len(data) != 0:\n",
    "        avgCite = sum(data.n_citation)/len(data)\n",
    "#         print(row.venue,\":\", avgCite)\n",
    "        AvgVenueCite.set_value(i,'avgVenueCite',avgCite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AvgVenueCite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.to_csv('Data/AvgVenueCitationFromTrainData_case_one.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply filter for case two experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Webscraped Aminer conference data\n",
    "##### from: http://aminer.org/ranks/conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminer = pd.read_csv('Data/Aminer_Conf_Ranks.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aminer = aminer[['Rank', 'Conference', 'H5-Index']]\n",
    "aminer.dropna(inplace=True)\n",
    "aminer.rename(index=str, columns={\"Conference\": \"venue\"}, inplace=True)\n",
    "aminer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed papers having NaN in any feature.\n",
    "data3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data based on known venue ranks and h5-index\n",
    "train_test = pd.merge(data3, aminer, how='left', on=['venue'])\n",
    "# train_test.drop(['Rank', 'H5-Index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped rows having NaN after merging with Aminer conference data\n",
    "train_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removed rank and h5-index to keep the dataset with only orignal feature before feature engineering.\n",
    "train_test.drop(['Rank', 'H5-Index'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droped duplicates papers if any\n",
    "train_test[train_test['id'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.drop_duplicates(['id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_test['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.to_csv('Data/train_test_case_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_two, test_two = train_test_split(train_test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_two.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_two.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_two.reset_index(drop=True, inplace=True)\n",
    "test_two.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_two.to_csv('Data/train_case_two.csv')\n",
    "test_two.to_csv('Data/test_case_two.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average citation count for venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VenueNames = train_two['venue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite = pd.DataFrame(VenueNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.rename(index=str, columns={0: \"venue\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.info(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in AvgVenueCite.iterrows():\n",
    "    data = train[train.venue == row.venue]\n",
    "    if len(data) != 0:\n",
    "        avgCite = sum(data.n_citation)/len(data)\n",
    "#         print(row.venue,\":\", avgCite)\n",
    "        AvgVenueCite.set_value(i,'avgVenueCite',avgCite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AvgVenueCite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgVenueCite.to_csv('Data/AvgVenueCitationFromTrainData_case_two.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
